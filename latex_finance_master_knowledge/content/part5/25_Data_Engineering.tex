\chapter{Data Engineering for Traders}\label{ch:dataeng}

\begin{abstract}
High‑quality data is the fuel of systematic trading.  We cover tick vs
bar aggregation, storage formats, vendor APIs, and real‑time pipelines
built with open‑source tooling.
\end{abstract}

\section{Tick, Quote, Trade}

\begin{description}
  \item[Tick] Event‑driven record of any book update.
  \item[Quote] Snapshot of top‑of‑book bid/ask at fixed interval.
  \item[Trade] Executed prints; basis for VWAP.
\end{description}

\section{Storage Choices}

\begin{itemize}
  \item **Columnar** (`Parquet`, `Feather`) – compressable, splittable.
  \item **Time‑series DB** (`kdb+`, `Influx`, `TimescaleDB`) – nanosecond indexing.
  \item **Object store** (S3, MinIO) + metadata catalogue (`Iceberg`, `Delta Lake`).
\end{itemize}

\section{Real‑Time Pipeline: Kafka → Flink → DuckDB}

\begin{minted}{bash}
# consume exchange feed, normalise JSON → Avro
flink run --jar normaliser.jar
# materialise 1‑min bars into DuckDB for back‑tester
\end{minted}

\section{Data Quality KPIs}

Missing rate, out‑of‑sequence messages, crossed‑book detection.

\section*{Key Takeaways}
\begin{itemize}
  \item Schema‑on‑read beats rigid relational models for heterogeneous feeds.
  \item Version control your data (Delta/Apache Iceberg) as strictly as code.
\end{itemize}
